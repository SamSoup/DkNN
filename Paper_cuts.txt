Prior approaches attempted to explain DNNs' predictions in several ways. Most work concentrates on post-hoc methods that provide explanations after a model has been trained \citep{madsen2022post}. For texts, oftentimes importance scores are computed per token to visualize and quantify each word's contribution towards the final decision \citep{wallace-etal-2018-interpreting, kennedy2020contextualizing, jayaram2021human, sikdar2021integrated}. Others attempt to elucidate the model's reasoning process by analogical justifications, which instead provide training instances similar to the input as example-based explanations \citep{papernot2018deep, lee2020improving}. Both can be useful depending on the application domains. For example, in health and law, decisions for new cases typically depend on historical precedents. In settings like this, example-based explanations may be more appropriate and intuitive to users in developing intuitions behind a model's inference procedure \citep{AYOUB2021102569, zhou2021evaluating}.

For efficiency, example representations can be pre-computed offline and cached after training completes, so that no additional computation is necessary during inference time. 

\subsection{Explanability, Interpretability, and Transparency in NLP}

Interpretability and explainability are multi-faceted concepts that have garnered extensive debate within the NLP community \citep{miller2019explanation, murdoch2019definitions, jacovi2020towards, linardatos2020explainable, molnar2021interpretable}. In line with  \citet{doshi2017towards}, we define interpretability as ``the ability to explain or to present in understandable terms to a human". While many, including ourselves, often refer to interpretability and explainability interchangeably, others argue that the latter is a more formal concept that necessitates establishing causality (Miller et al., 2019).

Our discussion of wrapper boxes primarily revolves around faithfulness, simplicity, and their 
trade-offs relative to model performance. However, these two principles certainly do not encompass the entire spectrum of what constitutes a good explanation. One crucial aspect to consider is \textit{utility}, which refers to the helpfulness of provided explanations (as judged by target task performance). Another closely related concept is model \textit{transparency}, which refers to the degree of visibility for said model's internal architecture. Overall, evaluation of these concepts is diverse and subjective \citep{sampson2019transparency}. Ultimately, the choice of specific metrics and evaluation procedures depends on the intended purpose and audience of posed explanations.
{
	"model_name_or_path": "./output/esnli/flan-t5-large/checkpoint-3500",
    "train_file": "./data/esnli/train_data.csv",
	"validation_file": "./data/esnli/train_data.csv",
	"test_file": "./data/esnli/test_data.csv",
	"evaluation_metrics": ["accuracy", "f1", "precision", "recall"],
	"seed": 42,
	"output_dir": "./output/esnli/flan-t5-large/train",
	"metric_for_best_model": "accuracy",
	"do_train_val_test_split": false,
	"overwrite_output_dir": true,
	"do_train": false,
	"do_eval": true,
	"do_predict": false,
	"compute_predict_results": false,
	"pad_to_max_length": false,
	"sentence1_key": "premise",
	"sentence2_key": "hypothesis",
	"use_fast_tokenizer": false,
	"report_to": "none",
	"int_to_text": ["entailment", "neutral", "contradiction"],
	"eval_accumulation_steps": 100,
	"per_device_train_batch_size": 1,
	"per_device_eval_batch_size": 1,
	"do_DKNN": false,
	"save_logits": false,
	"layers_to_save": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 
						16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 
						30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 
						44, 45, 46, 47, 48, 49],
	"poolers_to_use": ["mean_with_attention", "mean_with_attention", "mean_with_attention", 
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "mean_with_attention", "mean_with_attention",
                        "mean_with_attention", "flatten",
						"flatten", "flatten", "flatten", "flatten", "flatten", "flatten", 
						"flatten", "flatten", "flatten", "flatten", "flatten", "flatten", 
						"flatten", "flatten", "flatten", "flatten", "flatten", "flatten", 
						"flatten", "flatten", "flatten", "flatten", "flatten", "flatten"],
	"save_database_path": "./data/esnli/flan-t5-large-1/train/encoder_mean_with_attention_and_decoder_flatten"
}

{
	"config_name": "Samsoup/llama7B",
	"tokenizer_name": "Samsoup/llama7B",
	"model_name_or_path": "/scratch/06782/ysu707/output/esnli/llama7B-seed-42-dp-steps",
	"dataset_name": "Samsoup/esnli",
	"evaluation_metrics": ["accuracy", "f1", "precision", "recall"],
	"seed": 42,
	"output_dir": "/scratch/06782/ysu707/output/esnli/llama7B-seed-42-dp-steps/encodings",
    "metric_for_best_model": "accuracy",
	"do_train_val_test_split": false,
	"overwrite_output_dir": false,
	"do_train": false,
	"do_eval": false,
	"do_predict": false,
	"do_DKNN": false,
	"save_logits": false,
	"compute_predict_results": false,
	"pad_to_max_length": false,
    "do_compute_encodings": true,
	"sentence1_key": "premise",
	"sentence2_key": "hypothesis",
	"report_to": "none",
	"eval_accumulation_steps": 100,
	"per_device_eval_batch_size": 32,
	"layers_to_save": [
		0, 31, 32
	],
	"poolers_to_use": [
		"mean_with_attention", "mean_with_attention", "mean_with_attention"
	],
	"save_train_encodings_path": "/scratch/06782/ysu707/data/esnli/llama/train/mean_with_attention",
	"save_eval_encodings_path": "/scratch/06782/ysu707/data/esnli/llama/eval/mean_with_attention",
	"save_test_encodings_path": "/scratch/06782/ysu707/data/esnli/llama/test/mean_with_attention"
}

{
	"config_name": "Samsoup/llama7B",
	"tokenizer_name": "Samsoup/llama7B",
	"model_name_or_path": "/scratch/06782/ysu707/output/toxigen/llama-seed-42-dp-steps",
	"dataset_name": "Samsoup/toxigen",
	"evaluation_metrics": ["accuracy", "f1", "precision", "recall"],
	"seed": 42,
	"output_dir": "/scratch/06782/ysu707/output/toxigen/llama-seed-42-dp-steps/encodings",
    "metric_for_best_model": "f1",
	"do_train_val_test_split": false,
	"overwrite_output_dir": false,
	"do_train": false,
	"do_eval": false,
	"do_predict": false,
	"do_DKNN": false,
	"save_logits": false,
	"compute_predict_results": false,
	"pad_to_max_length": false,
    "do_compute_encodings": true,
	"sentence1_key": "text",
	"sentence2_key": "none",
	"report_to": "none",
	"eval_accumulation_steps": 100,
	"per_device_train_batch_size": 32,
	"per_device_eval_batch_size": 32,
	"layers_to_save": [
		0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 
		20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31
	],
	"poolers_to_use": [
		"mean_with_attention", "mean_with_attention", "mean_with_attention", 
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention", "mean_with_attention",
		"mean_with_attention", "mean_with_attention"
	],
	"save_train_encodings_path": "/scratch/06782/ysu707/data/toxigen/llama/train/mean_with_attention",
    "save_eval_encodings_path": "/scratch/06782/ysu707/data/toxigen/llama/eval/mean_with_attention",
	"save_test_encodings_path": "/scratch/06782/ysu707/data/toxigen/llama/test/mean_with_attention"
}
